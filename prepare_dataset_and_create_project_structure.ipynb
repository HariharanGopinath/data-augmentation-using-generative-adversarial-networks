{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Datasets and Create the Project Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook to preprocess the dataset and create a proper folder structure to store the data. This notebook must be run before running the `Data_Augmentation_Using_Generative_Adversarial_Networks.ipynb` notebook that should be available in the same folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esnure that the Cityscapes dataset is downloaded and placed in the current working directory. The dataset can be downloaded from [here](https://www.cityscapes-dataset.com/). You need to register to access the dataset. The following datasets are required for data augmentation:\n",
    "1. [gtFine_trainvaltest.zip](https://www.cityscapes-dataset.com/file-handling/?packageID=1) (241MB): Fine annotations for training and validation datasets (3475 annotated images) and dummy annotations (ignore regions) for the test set (1525 images).\n",
    "2. [leftImg8bit_trainvaltest.zip](https://www.cityscapes-dataset.com/file-handling/?packageID=3) (11GB): Left 8-bit images - training, validation, and test datasets (5000 images)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Python version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "assert (platform.python_version_tuple()[:2] >= ('3','7')), \"The notebooks are tested on Python 3.7 and higher. Please updated your Python to evaluate the code\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Notebook server has access to all required resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "dataset_folder = Path(\"dataset\")\n",
    "dataset_folder = Path.joinpath(Path.cwd(), dataset_folder)\n",
    "\n",
    "if not dataset_folder.exists():\n",
    "    raise FileNotFoundError(\"Add `{}` folder in the current directory (`{}`)\".format(dataset_folder.name, Path.cwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_zipped_datasets = [\"gtFine_trainvaltest.zip\", \"leftImg8bit_trainvaltest.zip\"]\n",
    "expected_zipped_datasets_path = list()\n",
    "\n",
    "for zipped_dataset in expected_zipped_datasets:\n",
    "    zipped_dataset = Path.joinpath(dataset_folder, zipped_dataset)\n",
    "    expected_zipped_datasets_path.append(zipped_dataset)\n",
    "    if not zipped_dataset.exists():\n",
    "        raise FileNotFoundError(\"Download and place `{}` in the current directory (`{}`)\".format(zipped_dataset.name, Path.cwd()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unzip Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile as zf\n",
    "\n",
    "unzipped_datasets_name = [str(dataset_name).replace(\".zip\", \"\") for dataset_name in expected_zipped_datasets]\n",
    "unzipped_datasets_path = [Path.joinpath(dataset_folder, dataset_name) for dataset_name in unzipped_datasets_name]\n",
    "\n",
    "for dataset_input_path, dataset_output_path in zip(expected_zipped_datasets_path, unzipped_datasets_path):\n",
    "    with zf.ZipFile(dataset_input_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(dataset_output_path)\n",
    "        \n",
    "print(\"Unzipped Datasets Successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Dataset: \n",
      "Found 5000 images of segmentation maps.\n",
      "Found 5000 photos.\n",
      "\n",
      "Training Dataset: \n",
      "Found 2975 images of segmentation maps.\n",
      "Found 2975 photos.\n",
      "\n",
      "Validation Dataset: \n",
      "Found 500 images of segmentation maps.\n",
      "Found 500 photos.\n",
      "\n",
      "Test Dataset: \n",
      "Found 1525 images of segmentation maps.\n",
      "Found 1525 photos.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "segmentation_map_expr_overall = str(unzipped_datasets_path[0]) + \"\\\\**\\\\*_color.png\"\n",
    "segmentation_map_paths_overall = glob.glob(segmentation_map_expr_overall, recursive=True)\n",
    "segmentation_map_paths_overall = sorted(segmentation_map_paths_overall)\n",
    "\n",
    "photo_expr_overall = str(unzipped_datasets_path[1]) + \"\\\\**\\\\*_leftImg8bit.png\"\n",
    "photo_paths_overall = glob.glob(photo_expr_overall, recursive=True)\n",
    "photo_overall = sorted(photo_paths_overall)\n",
    "\n",
    "print(\"\\nOverall Dataset: \\nFound {} images of segmentation maps.\\nFound {} photos.\".format(len(segmentation_map_paths_overall), len(segmentation_map_paths_overall)))\n",
    "\n",
    "segmentation_map_expr_train = str(unzipped_datasets_path[0]) + \"\\\\**\\\\train\\\\**\\\\*_color.png\"\n",
    "segmentation_map_paths_train = glob.glob(segmentation_map_expr_train, recursive=True)\n",
    "segmentation_map_paths_train = sorted(segmentation_map_paths_train)\n",
    "\n",
    "photo_expr_train = str(unzipped_datasets_path[1]) + \"\\\\**\\\\train\\\\**\\\\*_leftImg8bit.png\"\n",
    "photo_paths_train = glob.glob(photo_expr_train, recursive=True)\n",
    "photo_train = sorted(photo_paths_train)\n",
    "\n",
    "print(\"\\nTraining Dataset: \\nFound {} images of segmentation maps.\\nFound {} photos.\".format(len(segmentation_map_paths_train), len(photo_train)))\n",
    "\n",
    "segmentation_map_expr_val = str(unzipped_datasets_path[0]) + \"\\\\**\\\\val\\\\**\\\\*_color.png\"\n",
    "segmentation_map_paths_val = glob.glob(segmentation_map_expr_val, recursive=True)\n",
    "segmentation_map_paths_val = sorted(segmentation_map_paths_val)\n",
    "\n",
    "photo_expr_val = str(unzipped_datasets_path[1]) + \"\\\\**\\\\val\\\\**\\\\*_leftImg8bit.png\"\n",
    "photo_paths_val = glob.glob(photo_expr_val, recursive=True)\n",
    "photo_val = sorted(photo_paths_val)\n",
    "\n",
    "print(\"\\nValidation Dataset: \\nFound {} images of segmentation maps.\\nFound {} photos.\".format(len(segmentation_map_paths_val), len(photo_val)))\n",
    "\n",
    "segmentation_map_expr_test = str(unzipped_datasets_path[0]) + \"\\\\**\\\\test\\\\**\\\\*_color.png\"\n",
    "segmentation_map_paths_test = glob.glob(segmentation_map_expr_test, recursive=True)\n",
    "segmentation_map_paths_test = sorted(segmentation_map_paths_test)\n",
    "\n",
    "photo_expr_test = str(unzipped_datasets_path[1]) + \"\\\\**\\\\test\\\\**\\\\*_leftImg8bit.png\"\n",
    "photo_paths_test = glob.glob(photo_expr_test, recursive=True)\n",
    "photo_test = sorted(photo_paths_test)\n",
    "\n",
    "print(\"\\nTest Dataset: \\nFound {} images of segmentation maps.\\nFound {} photos.\".format(len(segmentation_map_paths_test), len(photo_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
