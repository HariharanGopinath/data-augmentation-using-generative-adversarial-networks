{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\01_Users\\01_Sourab_Bapu_Sridhar\\06_Projects\\Data-Augmentation-Using-Generative-Adversarial-Networks\n",
      "['input.zip']\n",
      "Unzipped Datasets Successfully!\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "from skimage import transform\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "import datetime\n",
    "import sys\n",
    "import os\n",
    "from imageio import imread\n",
    "\n",
    "#You can use this to find the directory for this file\n",
    "print(os.getcwd())\n",
    "\n",
    "#Sourab\n",
    "input_dir = \"D:/01_Users/01_Sourab_Bapu_Sridhar/06_Projects/Data-Augmentation-Using-Generative-Adversarial-Networks/dataset\"\n",
    "inputzip_dir = \"D:/01_Users/01_Sourab_Bapu_Sridhar/06_Projects/Data-Augmentation-Using-Generative-Adversarial-Networks/dataset/input.zip\"\n",
    "\n",
    "#Arens\n",
    "# input_dir = \"C:/Users/moose/OneDrive/Documents/GitHub/input\"\n",
    "#input_dir = \"/home/arenm/Data-Augmentation-Using-Generative-Adversarial-Networks/Dataset2\"\n",
    "#inputzip_dir = \"/home/arenm/Data-Augmentation-Using-Generative-Adversarial-Networks/Dataset2/input.zip\"\n",
    "print(os.listdir(input_dir))\n",
    "\n",
    "\n",
    "import zipfile as zf\n",
    "\n",
    "with zf.ZipFile(inputzip_dir, 'r') as zip_ref:\n",
    "    zip_ref.extractall(input_dir)\n",
    "        \n",
    "        \n",
    "print(\"Unzipped Datasets Successfully!\")\n",
    "\n",
    "input_dir = \"D:/01_Users/01_Sourab_Bapu_Sridhar/06_Projects/Data-Augmentation-Using-Generative-Adversarial-Networks/dataset/input\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  1 / 31\r"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": " Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node functional_3/conv2d_5/Conv2D (defined at <ipython-input-2-8a9527aa09d5>:246) ]] [Op:__inference_predict_function_2824]\n\nFunction call stack:\npredict_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-8a9527aa09d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    272\u001b[0m         \u001b[0mdiscriminator_acc_overall\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0md_loss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 274\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cityscapes\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_interval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-8a9527aa09d5>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(dataset_name, epochs, batch_size, show_interval)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m             \u001b[1;31m# Condition on B and generate a translated version\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 246\u001b[1;33m             \u001b[0mfake_A\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs_B\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m             \u001b[1;31m# Train the discriminators (original images = real / generated = Fake)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[0;32m    129\u001b[0m           method.__name__))\n\u001b[1;32m--> 130\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m   return tf_decorator.make_decorator(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1597\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1598\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1599\u001b[1;33m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1600\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1601\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    844\u001b[0m               *args, **kwds)\n\u001b[0;32m    845\u001b[0m       \u001b[1;31m# If we did not create any variables the trace we have is good enough.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 846\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_concrete_stateful_fn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcanon_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    847\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    848\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfn_with_cond\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minner_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0minner_kwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[0;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1924\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node functional_3/conv2d_5/Conv2D (defined at <ipython-input-2-8a9527aa09d5>:246) ]] [Op:__inference_predict_function_2824]\n\nFunction call stack:\npredict_function\n"
     ]
    }
   ],
   "source": [
    "def load_data(dataset_name, batch_size=1, is_val=False):\n",
    "    data_type = \"train\" if not is_val else \"val\"\n",
    "    path = glob(input_dir + '/%s/%s/%s/*' % (dataset_name, dataset_name, data_type))\n",
    "\n",
    "    batch_images = np.random.choice(path, size=batch_size)\n",
    "    img_res = (128, 128)\n",
    "    imgs_A = []\n",
    "    imgs_B = []\n",
    "    for img_path in batch_images:\n",
    "        img = imread(img_path)\n",
    "\n",
    "        h, w, _ = img.shape\n",
    "        _w = int(w / 2)\n",
    "        # because in the edges2shoes and maps dataset the input image comes before the ground truth.\n",
    "        if (dataset_name == \"edges2shoes\" or dataset_name == \"maps\"):\n",
    "            img_A, img_B = img[:, _w:, :], img[:, :_w, :]\n",
    "        else:\n",
    "            img_A, img_B = img[:, :_w, :], img[:, _w:, :]\n",
    "            \n",
    "\n",
    "        \n",
    "        # decreasing the resolution\n",
    "        img_A = transform.resize(img_A, img_res)  # Ground Truth image\n",
    "        img_B = transform.resize(img_B, img_res)  # Input image\n",
    "\n",
    "        # If training => do random flip , this is a trick to avoid overfitting\n",
    "        if not is_val and np.random.random() < 0.5:\n",
    "            img_A = np.fliplr(img_A)\n",
    "            img_B = np.fliplr(img_B)\n",
    "\n",
    "        imgs_A.append(img_A)\n",
    "        imgs_B.append(img_B)\n",
    "\n",
    "#     imgs_A = np.array(imgs_A) / 127.5 - 1.  # normalizing the images\n",
    "#     imgs_B = np.array(imgs_B) / 127.5 - 1.\n",
    "    \n",
    "\n",
    "    imgs_A = (imgs_A)/np.max(imgs_A) - 1\n",
    "    imgs_B = (imgs_B)/np.max(imgs_B) - 1\n",
    "\n",
    "    return imgs_A, imgs_B\n",
    "\n",
    "\n",
    "def load_batch(dataset_name, batch_size=1, is_val=False):\n",
    "    data_type = \"train\" if not is_val else \"val\"\n",
    "    path = glob(input_dir + '/%s/%s/%s/*' % (dataset_name, dataset_name, data_type))\n",
    "    n_batches = batch_size\n",
    "    img_res = (128, 128)\n",
    "    for i in range(n_batches - 1):\n",
    "        batch = path[i * batch_size:(i + 1) * batch_size]\n",
    "        imgs_A, imgs_B = [], []\n",
    "        for img in batch:\n",
    "            img = imread(img)\n",
    "            h, w, _ = img.shape\n",
    "            half_w = int(w / 2)\n",
    "            # because in the edges2shoes and maps dataset the input image comes before the ground truth.\n",
    "            if (dataset_name == \"edges2shoes\" or dataset_name == \"maps\"):\n",
    "                img_A, img_B = img[:, half_w:, :], img[:, :half_w, :]\n",
    "            else:\n",
    "                img_A, img_B = img[:, :half_w, :], img[:, half_w:, :]\n",
    "            img_A = transform.resize(img_A, img_res)  # Ground truth image\n",
    "            img_B = transform.resize(img_B, img_res)  # input image\n",
    "\n",
    "            # when training => do random flip , this is a trick to avoid overfitting\n",
    "            if not is_val and np.random.random() > 0.5:\n",
    "                img_A = np.fliplr(img_A)\n",
    "                img_B = np.fliplr(img_B)\n",
    "\n",
    "            imgs_A.append(img_A)\n",
    "            imgs_B.append(img_B)\n",
    "            \n",
    "        # normalizing the images\n",
    "#         imgs_A = np.array(imgs_A) / 127.5 - 1.\n",
    "#         imgs_B = np.array(imgs_B) / 127.5 - 1.\n",
    "        \n",
    "        imgs_A = (imgs_A)/np.max(imgs_A) - 1\n",
    "        imgs_B = (imgs_B)/np.max(imgs_B) - 1\n",
    "\n",
    "        yield imgs_A, imgs_B\n",
    "\n",
    "\n",
    "# def imread(path):\n",
    "#    return scipy.misc.imread(path, mode='RGB').astype(np.float)\n",
    "\n",
    "def build_generator():\n",
    "    \"\"\"U-Net Generator\"\"\"\n",
    "\n",
    "    def conv2d(layer_input, filters, f_size=4, bn=True):\n",
    "        \"\"\"Layers used during downsampling\"\"\"\n",
    "        d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "        d = LeakyReLU(alpha=0.2)(d)\n",
    "        if bn:\n",
    "            d = BatchNormalization(momentum=0.8)(d)\n",
    "        return d\n",
    "\n",
    "    def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n",
    "        \"\"\"Layers used during upsampling\"\"\"\n",
    "        u = UpSampling2D(size=2)(layer_input)\n",
    "        u = Conv2D(filters, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)\n",
    "        if dropout_rate:\n",
    "            u = Dropout(dropout_rate)(u)\n",
    "        u = BatchNormalization(momentum=0.8)(u)\n",
    "        u = Concatenate()([u, skip_input])  # skip connection\n",
    "        return u\n",
    "\n",
    "    d0 = Input(shape=img_shape)\n",
    "\n",
    "    # Downsampling\n",
    "    d1 = conv2d(d0, gf, bn=False)\n",
    "    d2 = conv2d(d1, gf * 2)\n",
    "    d3 = conv2d(d2, gf * 4)\n",
    "    d4 = conv2d(d3, gf * 8)\n",
    "    d5 = conv2d(d4, gf * 8)\n",
    "    d6 = conv2d(d5, gf * 8)\n",
    "    d7 = conv2d(d6, gf * 8)\n",
    "\n",
    "    # Upsampling\n",
    "    u1 = deconv2d(d7, d6, gf * 8)\n",
    "    u2 = deconv2d(u1, d5, gf * 8)\n",
    "    u3 = deconv2d(u2, d4, gf * 8)\n",
    "    u4 = deconv2d(u3, d3, gf * 4)\n",
    "    u5 = deconv2d(u4, d2, gf * 2)\n",
    "    u6 = deconv2d(u5, d1, gf)\n",
    "\n",
    "    u7 = UpSampling2D(size=2)(u6)\n",
    "    output_img = Conv2D(channels, kernel_size=4, strides=1, padding='same', activation='tanh')(u7)\n",
    "\n",
    "    return Model(d0, output_img)\n",
    "\n",
    "\n",
    "def build_discriminator():\n",
    "    # a small function to make one layer of the discriminator\n",
    "    def d_layer(layer_input, filters, f_size=4, bn=True):\n",
    "        \"\"\"Discriminator layer\"\"\"\n",
    "        d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "        d = LeakyReLU(alpha=0.2)(d)\n",
    "        if bn:\n",
    "            d = BatchNormalization(momentum=0.8)(d)\n",
    "        return d\n",
    "\n",
    "    img_A = Input(shape=img_shape)\n",
    "    img_B = Input(shape=img_shape)\n",
    "\n",
    "    # Concatenate image and conditioning image by channels to produce input\n",
    "    combined_imgs = Concatenate(axis=-1)([img_A, img_B])\n",
    "\n",
    "    d1 = d_layer(combined_imgs, df, bn=False)\n",
    "    d2 = d_layer(d1, df * 2)\n",
    "    d3 = d_layer(d2, df * 4)\n",
    "    d4 = d_layer(d3, df * 8)\n",
    "\n",
    "    validity = Conv2D(1, kernel_size=4, strides=1, padding='same')(d4)\n",
    "\n",
    "    return Model([img_A, img_B], validity)\n",
    "\n",
    "\n",
    "# Input shape\n",
    "img_rows = 128\n",
    "img_cols = 128\n",
    "channels = 3\n",
    "img_shape = (img_rows, img_cols, channels)\n",
    "\n",
    "# Calculate output shape of D (PatchGAN)\n",
    "patch = int(img_rows / 2 ** 4)\n",
    "disc_patch = (patch, patch, 1)\n",
    "\n",
    "# Number of filters in the first layer of G and D\n",
    "gf = 64\n",
    "df = 64\n",
    "\n",
    "optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "# Build and compile the discriminator\n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(loss='mse',\n",
    "                      optimizer=optimizer,\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "# Build the generator\n",
    "generator = build_generator()\n",
    "\n",
    "# Input images and their conditioning images\n",
    "img_A = Input(shape=img_shape)\n",
    "img_B = Input(shape=img_shape)\n",
    "\n",
    "# By conditioning on B generate a fake version of A\n",
    "fake_A = generator(img_B)\n",
    "\n",
    "# For the combined model we will only train the generator\n",
    "discriminator.trainable = False\n",
    "\n",
    "# Discriminators determines validity of translated images / condition pairs\n",
    "valid = discriminator([fake_A, img_B])\n",
    "\n",
    "combined = Model(inputs=[img_A, img_B], outputs=[valid, fake_A])\n",
    "combined.compile(loss=['mse', 'mae'],\n",
    "                 loss_weights=[1, 100],\n",
    "                 optimizer=optimizer)\n",
    "\n",
    "# Initialize Generaor and Discriminator Losses\n",
    "generator_loss_overall = []\n",
    "discriminator_loss_overall = []\n",
    "\n",
    "discriminator_acc_overall = []\n",
    "\n",
    "\n",
    "def show_images(dataset_name, epoch, batch_i):\n",
    "    r, c = 3, 3\n",
    "\n",
    "    imgs_A, imgs_B = load_data(dataset_name, batch_size=3, is_val=True)\n",
    "    fake_A = generator.predict(imgs_B)\n",
    "\n",
    "    gen_imgs = np.concatenate([imgs_B, fake_A, imgs_A])\n",
    "\n",
    "    # Rescale images 0 - 1\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "    titles = ['Input', 'Output', 'Ground Truth']\n",
    "    fig, axs = plt.subplots(r, c, figsize=(20,20))\n",
    "    cnt = 0\n",
    "    \n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            gen_imgs[cnt] *= 1.0/np.max(gen_imgs[cnt])\n",
    "            axs[i, j].imshow(gen_imgs[cnt])\n",
    "            axs[i, j].set_title(titles[i])\n",
    "            axs[i, j].axis('off')\n",
    "            cnt += 1\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def train(dataset_name, epochs, batch_size=1, show_interval=10):\n",
    "    start_time = datetime.datetime.now()\n",
    "    \n",
    "    # Adversarial loss ground truths\n",
    "    valid = np.ones((batch_size,) + disc_patch)\n",
    "    fake = np.zeros((batch_size,) + disc_patch)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch_i, (imgs_A, imgs_B) in enumerate(load_batch(dataset_name, batch_size)):\n",
    "            #  Train Discriminator\n",
    "            print(\"Batch: \",batch_i+1,\"/\",batch_size-1, end='\\r')\n",
    "            \n",
    "            # Condition on B and generate a translated version\n",
    "            fake_A = generator.predict(imgs_B)            \n",
    "            \n",
    "            # Train the discriminators (original images = real / generated = Fake)\n",
    "            d_loss_real = discriminator.train_on_batch([imgs_A, imgs_B], valid)\n",
    "            d_loss_fake = discriminator.train_on_batch([fake_A, imgs_B], fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            #  Train Generator\n",
    "            g_loss = combined.train_on_batch([imgs_A, imgs_B], [valid, imgs_A])\n",
    "\n",
    "            elapsed_time = datetime.datetime.now() - start_time\n",
    "\n",
    "        # Plot the progress\n",
    "        if epoch % 1 == 0:\n",
    "            print(\"[Epoch %d/%d]  [D loss: %f, acc: %3d%%] [G loss: %f] time: %s\" % (epoch, epochs,\n",
    "\n",
    "                                                                                     d_loss[0], 100 * d_loss[1],\n",
    "                                                                                     g_loss[0],\n",
    "                                                                                     elapsed_time))\n",
    "        # If at show interval => show generated image samples\n",
    "        if epoch % show_interval == 0:\n",
    "            show_images(dataset_name, epoch, batch_i)\n",
    "        # Initialize Generaor and Discriminator Losses\n",
    "        generator_loss_overall.append(g_loss[0])\n",
    "        discriminator_loss_overall.append(d_loss[0])\n",
    "\n",
    "        discriminator_acc_overall.append(100 * d_loss[1])\n",
    "\n",
    "train(\"cityscapes\", epochs=1, batch_size=32, show_interval=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "def show_images(dataset_name, epoch, batch_i):\n",
    "    r, c = 3, 3\n",
    "\n",
    "    imgs_A, imgs_B = load_data(dataset_name, batch_size=3, is_val=True)\n",
    "    fake_A = generator.predict(imgs_B)\n",
    "    gen_imgs = np.concatenate([imgs_B, fake_A, imgs_A])\n",
    "\n",
    "    # Rescale images 0 - 1\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "    titles = ['Input', 'Output', 'Ground Truth']\n",
    "    fig, axs = plt.subplots(r, c, figsize=(20,20))\n",
    "    cnt = 0\n",
    "\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            gen_imgs[cnt] *= 1.0/np.max(gen_imgs[cnt])\n",
    "            axs[i, j].imshow(gen_imgs[cnt])\n",
    "            axs[i, j].set_title(titles[i])\n",
    "            axs[i, j].axis('off')\n",
    "            cnt += 1\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "show_images(\"cityscapes\", 0, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator_loss_overall\n",
    "# discriminator_loss_overall\n",
    "\n",
    "# discriminator_acc_overall\n",
    "\n",
    "# fig, ax = plt.subplots(ncols=2, figsize=(15,4))\n",
    "# plt.ion()\n",
    "plt.plot(generator_loss_overall)\n",
    "\n",
    "# Add legends and labels\n",
    "plt.title('Generator Loss')\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ax[1].set_title('Discriminator Loss')\n",
    "# ax[1].set_xlabel('Number of epochs')\n",
    "# ax[1].set_ylabel('Loss')\n",
    "# ax[0].legend(['Train', 'Validation'])\n",
    "\n",
    "# Draw the figure on the screen\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ax[0].legend(['Tringing'])\n",
    "# plt.cl\n",
    "plt.plot(discriminator_loss_overall)\n",
    "\n",
    "plt.title('Discriminator Loss')\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.clear()\n",
    "plt.plot(discriminator_acc_overall)\n",
    "\n",
    "plt.title('Discriminator Accuracy')\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.ylabel('Per cent')\n",
    "plt.show()\n",
    "# ax[1].legend(['Train', 'Validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
