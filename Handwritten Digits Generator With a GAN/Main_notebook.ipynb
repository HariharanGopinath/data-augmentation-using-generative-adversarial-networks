{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import transform\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "import datetime\n",
    "import sys\n",
    "import os\n",
    "from imageio import imread\n",
    "\n",
    "print(os.getcwd())\n",
    "#Azure. This means that you have the provided dataset in the Github folder\n",
    "input_dir = \"/home/arenm/Project/directory to extract/input\"\n",
    "\n",
    "#Arens\n",
    "#input_dir = \"C:/Users/moose/OneDrive/Documents/GitHub/input\"\n",
    "print(os.listdir(input_dir))\n",
    "\n",
    "\n",
    "def load_data(dataset_name, batch_size=1, is_val=False):\n",
    "    data_type = \"train\" if not is_val else \"val\"\n",
    "    path = glob(input_dir + '/%s/%s/%s/*' % (dataset_name, dataset_name, data_type))\n",
    "\n",
    "    batch_images = np.random.choice(path, size=batch_size)\n",
    "    img_res = (128, 128)\n",
    "    imgs_A = []\n",
    "    imgs_B = []\n",
    "    for img_path in batch_images:\n",
    "        img = imread(img_path)\n",
    "\n",
    "        h, w, _ = img.shape\n",
    "        _w = int(w / 2)\n",
    "        # because in the edges2shoes and maps dataset the input image comes before the ground truth.\n",
    "        if (dataset_name == \"edges2shoes\" or dataset_name == \"maps\"):\n",
    "            img_A, img_B = img[:, _w:, :], img[:, :_w, :]\n",
    "        else:\n",
    "            img_A, img_B = img[:, :_w, :], img[:, _w:, :]\n",
    "        # decreasing the resolution\n",
    "        img_A = transform.resize(img_A, img_res)  # Ground Truth image\n",
    "        img_B = transform.resize(img_B, img_res)  # Input image\n",
    "\n",
    "        # If training => do random flip , this is a trick to avoid overfitting\n",
    "        if not is_val and np.random.random() < 0.5:\n",
    "            img_A = np.fliplr(img_A)\n",
    "            img_B = np.fliplr(img_B)\n",
    "\n",
    "        imgs_A.append(img_A)\n",
    "        imgs_B.append(img_B)\n",
    "\n",
    "    imgs_A = np.array(imgs_A) / 127.5 - 1.  # normalizing the images\n",
    "    imgs_B = np.array(imgs_B) / 127.5 - 1.\n",
    "\n",
    "    return imgs_A, imgs_B\n",
    "\n",
    "\n",
    "def load_batch(dataset_name, batch_size=1, is_val=False):\n",
    "    data_type = \"train\" if not is_val else \"val\"\n",
    "    path = glob(input_dir + '/%s/%s/%s/*' % (dataset_name, dataset_name, data_type))\n",
    "    n_batches = batch_size\n",
    "    img_res = (128, 128)\n",
    "    for i in range(n_batches - 1):\n",
    "        batch = path[i * batch_size:(i + 1) * batch_size]\n",
    "        imgs_A, imgs_B = [], []\n",
    "        for img in batch:\n",
    "            img = imread(img)\n",
    "            h, w, _ = img.shape\n",
    "            half_w = int(w / 2)\n",
    "            # because in the edges2shoes and maps dataset the input image comes before the ground truth.\n",
    "            if (dataset_name == \"edges2shoes\" or dataset_name == \"maps\"):\n",
    "                img_A, img_B = img[:, half_w:, :], img[:, :half_w, :]\n",
    "            else:\n",
    "                img_A, img_B = img[:, :half_w, :], img[:, half_w:, :]\n",
    "            img_A = transform.resize(img_A, img_res)  # Ground truth image\n",
    "            img_B = transform.resize(img_B, img_res)  # input image\n",
    "\n",
    "            # when training => do random flip , this is a trick to avoid overfitting\n",
    "            if not is_val and np.random.random() > 0.5:\n",
    "                img_A = np.fliplr(img_A)\n",
    "                img_B = np.fliplr(img_B)\n",
    "\n",
    "            imgs_A.append(img_A)\n",
    "            imgs_B.append(img_B)\n",
    "        # normalizing the images\n",
    "        imgs_A = np.array(imgs_A) / 127.5 - 1.\n",
    "        imgs_B = np.array(imgs_B) / 127.5 - 1.\n",
    "\n",
    "        yield imgs_A, imgs_B\n",
    "\n",
    "\n",
    "# def imread(path):\n",
    "#    return scipy.misc.imread(path, mode='RGB').astype(np.float)\n",
    "\n",
    "def build_generator():\n",
    "    \"\"\"U-Net Generator\"\"\"\n",
    "\n",
    "    def conv2d(layer_input, filters, f_size=4, bn=True):\n",
    "        \"\"\"Layers used during downsampling\"\"\"\n",
    "        d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "        d = LeakyReLU(alpha=0.2)(d)\n",
    "        if bn:\n",
    "            d = BatchNormalization(momentum=0.8)(d)\n",
    "        return d\n",
    "\n",
    "    def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n",
    "        \"\"\"Layers used during upsampling\"\"\"\n",
    "        u = UpSampling2D(size=2)(layer_input)\n",
    "        u = Conv2D(filters, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)\n",
    "        if dropout_rate:\n",
    "            u = Dropout(dropout_rate)(u)\n",
    "        u = BatchNormalization(momentum=0.8)(u)\n",
    "        u = Concatenate()([u, skip_input])  # skip connection\n",
    "        return u\n",
    "\n",
    "    d0 = Input(shape=img_shape)\n",
    "\n",
    "    # Downsampling\n",
    "    d1 = conv2d(d0, gf, bn=False)\n",
    "    d2 = conv2d(d1, gf * 2)\n",
    "    d3 = conv2d(d2, gf * 4)\n",
    "    d4 = conv2d(d3, gf * 8)\n",
    "    d5 = conv2d(d4, gf * 8)\n",
    "    d6 = conv2d(d5, gf * 8)\n",
    "    d7 = conv2d(d6, gf * 8)\n",
    "\n",
    "    # Upsampling\n",
    "    u1 = deconv2d(d7, d6, gf * 8)\n",
    "    u2 = deconv2d(u1, d5, gf * 8)\n",
    "    u3 = deconv2d(u2, d4, gf * 8)\n",
    "    u4 = deconv2d(u3, d3, gf * 4)\n",
    "    u5 = deconv2d(u4, d2, gf * 2)\n",
    "    u6 = deconv2d(u5, d1, gf)\n",
    "\n",
    "    u7 = UpSampling2D(size=2)(u6)\n",
    "    output_img = Conv2D(channels, kernel_size=4, strides=1, padding='same', activation='tanh')(u7)\n",
    "\n",
    "    return Model(d0, output_img)\n",
    "\n",
    "\n",
    "def build_discriminator():\n",
    "    # a small function to make one layer of the discriminator\n",
    "    def d_layer(layer_input, filters, f_size=4, bn=True):\n",
    "        \"\"\"Discriminator layer\"\"\"\n",
    "        d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "        d = LeakyReLU(alpha=0.2)(d)\n",
    "        if bn:\n",
    "            d = BatchNormalization(momentum=0.8)(d)\n",
    "        return d\n",
    "\n",
    "    img_A = Input(shape=img_shape)\n",
    "    img_B = Input(shape=img_shape)\n",
    "\n",
    "    # Concatenate image and conditioning image by channels to produce input\n",
    "    combined_imgs = Concatenate(axis=-1)([img_A, img_B])\n",
    "\n",
    "    d1 = d_layer(combined_imgs, df, bn=False)\n",
    "    d2 = d_layer(d1, df * 2)\n",
    "    d3 = d_layer(d2, df * 4)\n",
    "    d4 = d_layer(d3, df * 8)\n",
    "\n",
    "    validity = Conv2D(1, kernel_size=4, strides=1, padding='same')(d4)\n",
    "\n",
    "    return Model([img_A, img_B], validity)\n",
    "\n",
    "\n",
    "# Input shape\n",
    "img_rows = 128\n",
    "img_cols = 128\n",
    "channels = 3\n",
    "img_shape = (img_rows, img_cols, channels)\n",
    "\n",
    "# Calculate output shape of D (PatchGAN)\n",
    "patch = int(img_rows / 2 ** 4)\n",
    "disc_patch = (patch, patch, 1)\n",
    "\n",
    "# Number of filters in the first layer of G and D\n",
    "gf = 64\n",
    "df = 64\n",
    "\n",
    "optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "# Build and compile the discriminator\n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(loss='mse',\n",
    "                      optimizer=optimizer,\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "# Build the generator\n",
    "generator = build_generator()\n",
    "\n",
    "# Input images and their conditioning images\n",
    "img_A = Input(shape=img_shape)\n",
    "img_B = Input(shape=img_shape)\n",
    "\n",
    "# By conditioning on B generate a fake version of A\n",
    "fake_A = generator(img_B)\n",
    "\n",
    "# For the combined model we will only train the generator\n",
    "discriminator.trainable = False\n",
    "\n",
    "# Discriminators determines validity of translated images / condition pairs\n",
    "valid = discriminator([fake_A, img_B])\n",
    "\n",
    "combined = Model(inputs=[img_A, img_B], outputs=[valid, fake_A])\n",
    "combined.compile(loss=['mse', 'mae'],\n",
    "                 loss_weights=[1, 100],\n",
    "                 optimizer=optimizer)\n",
    "\n",
    "\n",
    "def show_images(dataset_name, epoch, batch_i):\n",
    "    r, c = 3, 3\n",
    "\n",
    "    imgs_A, imgs_B = load_data(dataset_name, batch_size=3, is_val=True)\n",
    "    fake_A = generator.predict(imgs_B)\n",
    "\n",
    "    gen_imgs = np.concatenate([imgs_B, fake_A, imgs_A])\n",
    "\n",
    "    # Rescale images 0 - 1\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "    titles = ['Input', 'Output', 'Ground Truth']\n",
    "    fig, axs = plt.subplots(r, c)\n",
    "    cnt = 0\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i, j].imshow(gen_imgs[cnt])\n",
    "            axs[i, j].set_title(titles[i])\n",
    "            axs[i, j].axis('off')\n",
    "            cnt += 1\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def train(dataset_name, epochs, batch_size=1, show_interval=10):\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    # Adversarial loss ground truths\n",
    "    valid = np.ones((batch_size,) + disc_patch)\n",
    "    fake = np.zeros((batch_size,) + disc_patch)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch_i, (imgs_A, imgs_B) in enumerate(load_batch(dataset_name, batch_size)):\n",
    "            #  Train Discriminator\n",
    "\n",
    "            # Condition on B and generate a translated version\n",
    "            fake_A = generator.predict(imgs_B)\n",
    "\n",
    "            # Train the discriminators (original images = real / generated = Fake)\n",
    "            d_loss_real = discriminator.train_on_batch([imgs_A, imgs_B], valid)\n",
    "            d_loss_fake = discriminator.train_on_batch([fake_A, imgs_B], fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            #  Train Generator\n",
    "            g_loss = combined.train_on_batch([imgs_A, imgs_B], [valid, imgs_A])\n",
    "\n",
    "            elapsed_time = datetime.datetime.now() - start_time\n",
    "\n",
    "        # Plot the progress\n",
    "        if epoch % 1 == 0:\n",
    "            print(\"[Epoch %d/%d]  [D loss: %f, acc: %3d%%] [G loss: %f] time: %s\" % (epoch, epochs,\n",
    "\n",
    "                                                                                     d_loss[0], 100 * d_loss[1],\n",
    "                                                                                     g_loss[0],\n",
    "                                                                                     elapsed_time))\n",
    "        # If at show interval => show generated image samples\n",
    "        if epoch % show_interval == 0:\n",
    "            show_images(dataset_name, epoch, batch_i)\n",
    "\n",
    "\n",
    "train(\"cityscapes\", epochs=50, batch_size=32, show_interval=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
