{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation Using Generative Adversarial Networks (GANs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook to perform data augmentation using Generative Adversarial Networks (GANs). This notebook must be run after running the notebook `prepare_dataset_and_create_project_structure.ipynb` that should be available in the same directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Ensure that the Cityscapes dataset is downloaded and placed in the directory named **dataset**. More information about the dataset can be found in the notebook `prepare_dataset_and_create_project_structure.ipynb` that should be available in the same directory. \n",
    "2. Run all the cells in the notebook `prepare_dataset_and_create_project_structure.ipynb` to understand the dataset, process the dataset, create the project structure required to ensure correct results from the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Check Python version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is crucial to ensure that the notebook runs on the correct version of Python to guarantee proper functionality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "assert (platform.python_version_tuple()[:2] >= ('3','7')), \"[ERROR] The notebooks are tested on Python 3.7 and higher. Please updated your Python to evaluate the code\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Check Notebook server has access to all required resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "dataset_folder = Path(\"dataset\")\n",
    "dataset_folder = Path.joinpath(Path.cwd(), dataset_folder)\n",
    "\n",
    "if not dataset_folder.exists():\n",
    "    raise FileNotFoundError(\"[ERROR] Add `{}` folder in the current directory (`{}`)\".format(dataset_folder.name, Path.cwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_preparation_notebook = Path(\"prepare_dataset_and_create_project_structure.ipynb\")\n",
    "dataset_preparation_notebook = Path.joinpath(Path.cwd(), dataset_preparation_notebook)\n",
    "\n",
    "if not dataset_preparation_notebook.exists():\n",
    "    raise FileNotFoundError(\"[ERROR] The notebook `{}` is unavailable in the current directory (`{}`). Please download and run the notebook `{}` before this notebook to ensure proper results.\".format(dataset_preparation_notebook.name, Path.cwd(), dataset_preparation_notebook.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = Path.joinpath(dataset_folder, \"test_dataset\")\n",
    "test_dataset_A = Path.joinpath(test_dataset, \"A\")\n",
    "test_dataset_A = Path.joinpath(test_dataset, \"B\")\n",
    "\n",
    "test_dataset_overall = [test_dataset, test_dataset_A, test_dataset_B]\n",
    "\n",
    "for dataset in test_dataset_overall:\n",
    "    if not dataset.exists():\n",
    "        raise FileNotFoundError(\"[ERROR] The folder `{}` is unavailable. Please run the notebook `prepare_dataset_and_create_project_structure.ipynb` available in the current directory (`{}`) before running this notebook.\".format(dataset.name, Path.cwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = Path.joinpath(dataset_folder, \"training_dataset\")\n",
    "training_dataset_A = Path.joinpath(training_dataset, \"A\")\n",
    "training_dataset_B = Path.joinpath(training_dataset, \"B\")\n",
    "\n",
    "training_dataset_overall = [training_dataset, training_dataset_A, training_dataset_B]\n",
    "\n",
    "for dataset in training_dataset_overall:\n",
    "    if not dataset.exists():\n",
    "        raise FileNotFoundError(\"[ERROR] The folder `{}` is unavailable. Please run the notebook `prepare_dataset_and_create_project_structure.ipynb` available in the current directory (`{}`) before running this notebook.\".format(dataset.name, Path.cwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validatation_dataset = Path.joinpath(dataset_folder, \"validatation_dataset\")\n",
    "validatation_dataset_A = Path.joinpath(validatation_dataset, \"A\")\n",
    "validatation_dataset_B = Path.joinpath(validatation_dataset, \"B\")\n",
    "\n",
    "validatation_dataset_overall = [validatation_dataset, validatation_dataset_A, validatation_dataset_B]\n",
    "\n",
    "for dataset in validatation_dataset_overall:\n",
    "    if not dataset.exists():\n",
    "        raise FileNotFoundError(\"[ERROR] The folder `{}` is unavailable. Please run the notebook `prepare_dataset_and_create_project_structure.ipynb` available in the current directory (`{}`) before running this notebook.\".format(dataset.name, Path.cwd()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the biggest bottlenecks in creating generalized deep learning models is a scarcity of high-quality data. The collection of high-quality data and its conversion is expensive. Most of the data collection methods are labor-intensive and error-prone, requiring considerable editing afterward to clean the data. Since large amounts of data are needed to achieve generalized deep learning models, standard data augmentation methods are routinely used to increase the dataset's generalizability. Data augmentation methods are also used when the datasets are imbalanced, improving the model's overall performance.\n",
    "\n",
    "Generative Adversarial Networks, popularly known as GANs, are a novel method for data augmentation. The generation of artificial training data can not only be instrumental in situations such as imbalanced data sets, but it can also be useful when the original dataset contains sensitive information. In such cases, it is then desirable to avoid using the original data as much as possible (For example, Medical data).\n",
    "\n",
    "This project proposes a GAN architecture based on this [paper](https://arxiv.org/abs/1611.07004) to perform data augmentation using the popular image-to-image translation method. Generative Adversarial Networks trained on these methods learn the mapping from an input image to an output image and learn a loss function to train this mapping. Therefore, this approach makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. In this particular project, we demonstrate that this approach can effectively synthesize photos from label maps. To evaluate the performance of the proposed GAN architecture, we utilize a standard dataset named Cityscapes. The Cityscapes Dataset focuses on semantic understanding of urban street scenes. The dataset contains 5000 images with detailed annotations and 20000 images with coarse annotations apart from the original images. Some sample images from the dataset are presented below:\n",
    "\n",
    "![Sample Image from Cityscape Dataset](https://www.cityscapes-dataset.com/wordpress/wp-content/uploads/2015/07/stuttgart02-2040x500.png)\n",
    "![Sample Image from Cityscape Dataset](https://www.cityscapes-dataset.com/wordpress/wp-content/uploads/2015/07/stuttgart00-2040x500.png)\n",
    "![Sample Image from Cityscape Dataset](https://www.cityscapes-dataset.com/wordpress/wp-content/uploads/2015/07/stuttgart04-2040x500.png)\n",
    "![Sample Image from Cityscape Dataset](https://www.cityscapes-dataset.com/wordpress/wp-content/uploads/2015/07/stuttgart01-2040x500.png)\n",
    "\n",
    "<center>Image Courtesy: Cityscapes Datatset (Link: https://www.cityscapes-dataset.com/)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Background Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the advancements in deep learning, the most striking successes have involved discriminative models, usually those that map a high-dimensional, rich sensory input to a class label. These striking successes have primarily been based on the backpropagation and dropout algorithms, using piecewise linear units. However, deep generative models have had less impact due to the challenges of approximating many probabilistic computations that occur due to the usage of piecewise linear units in the generative context. \n",
    "\n",
    "Generative Adversarial Networks, popularly known as GANs, is a machine learning framework class that sidesteps these difficulties by pitting the generative model against an adversary. In other words, a GAN is a machine learning framework where two neural networks compete against each other in a zero-sum game (i.e., one network's gain is the other network's loss). The two networks in a GAN can be considered as a generator and a discriminator. The generator learns to create images that look real, while the discriminator learns to tell real images apart from fakes. Competition in this game drives both networks to improve their models until the counterfeits are indistinguishable from the genuine images. An overview of the Generative Adversarial Network is represented below:\n",
    "\n",
    "![Overview of Generative Adversarial Network](https://developers.google.com/machine-learning/gan/images/gan_diagram.svg)\n",
    "\n",
    "<center>Image Courtesy: Google Developers (Link: https://developers.google.com/machine-learning/gan/gan_structure)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Proposed Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import datetime\n",
    "import numpy as np \n",
    "import scipy as sp\n",
    "from imageio import imread\n",
    "from skimage import transform\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load data\n",
    "\n",
    "def load_data(mode):\n",
    "    \n",
    "    if(mode == \"training\"):\n",
    "        dataset_expr = str(training_dataset) + \"\\\\**\\\\*.jpg\"\n",
    "    else:\n",
    "        dataset_expr = str(validatation_dataset) + \"\\\\**\\\\*.jpg\"\n",
    "        \n",
    "    dataset_paths = glob.glob(dataset_expr, recursive=True)\n",
    "    dataset_paths = sorted(dataset_paths)\n",
    "    \n",
    "    return dataset_paths\n",
    "    \n",
    "print(\"******************\")\n",
    "test = load_data(\"training\")\n",
    "print(test)\n",
    "\n",
    "print(\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\")\n",
    "\n",
    "print(\"******************\")\n",
    "test = load_data(\"validation\")\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset_name,batch_size=1, is_val=False):\n",
    "        data_type = \"train\" if not is_val else \"val\"\n",
    "        path = glob('../input/%s/%s/%s/*' % (dataset_name,dataset_name, data_type))\n",
    "\n",
    "        batch_images = np.random.choice(path, size=batch_size)\n",
    "        img_res=(128,128)\n",
    "        imgs_A = []\n",
    "        imgs_B = []\n",
    "        for img_path in batch_images:\n",
    "            img = imread(img_path)\n",
    "\n",
    "            h, w, _ = img.shape\n",
    "            _w = int(w/2)\n",
    "            # because in the edges2shoes and maps dataset the input image comes before the ground truth.\n",
    "            if (dataset_name==\"edges2shoes\" or dataset_name==\"maps\"):\n",
    "                img_A, img_B = img[:, _w:, :],img[:, :_w, :] \n",
    "            else:  \n",
    "                img_A, img_B = img[:, :_w, :], img[:, _w:, :]\n",
    "            # decreasing the resolution \n",
    "            img_A = transform.resize(img_A, img_res)  #Ground Truth image\n",
    "            img_B = transform.resize(img_B, img_res)  #Input image\n",
    "\n",
    "            # If training => do random flip , this is a trick to avoid overfitting \n",
    "            if not is_val and np.random.random() < 0.5:\n",
    "                img_A = np.fliplr(img_A)\n",
    "                img_B = np.fliplr(img_B)\n",
    "\n",
    "            imgs_A.append(img_A)\n",
    "            imgs_B.append(img_B)\n",
    "            \n",
    "        \n",
    "        imgs_A = np.array(imgs_A)/127.5 - 1.  #normalizing the images\n",
    "        imgs_B = np.array(imgs_B)/127.5 - 1.\n",
    "\n",
    "        return imgs_A, imgs_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_batch( dataset_name,batch_size=1, is_val=False):\n",
    "        data_type = \"train\" if not is_val else \"val\"\n",
    "        path = glob('../input/%s/%s/%s/*' % (dataset_name,dataset_name, data_type))\n",
    "\n",
    "        \n",
    "        n_batches=batch_size\n",
    "        img_res=(128,128)\n",
    "        for i in range(n_batches-1):\n",
    "            batch = path[i*batch_size:(i+1)*batch_size]\n",
    "            imgs_A, imgs_B = [], []\n",
    "            for img in batch:\n",
    "                img = imread(img)\n",
    "                h, w, _ = img.shape\n",
    "                half_w = int(w/2)\n",
    "                # because in the edges2shoes and maps dataset the input image comes before the ground truth.\n",
    "                if (dataset_name==\"edges2shoes\"or dataset_name==\"maps\"):\n",
    "                      img_A, img_B = img[:, half_w:, :],img[:, :half_w, :] \n",
    "                else:  \n",
    "                      img_A, img_B = img[:, :half_w, :], img[:, half_w:, :]\n",
    "                img_A = transform.resize(img_A, img_res)#Ground truth image\n",
    "                img_B = transform.resize(img_B, img_res)# input image\n",
    "                \n",
    " # when training => do random flip , this is a trick to avoid overfitting \n",
    "                if not is_val and np.random.random() > 0.5:\n",
    "                        img_A = np.fliplr(img_A)\n",
    "                        img_B = np.fliplr(img_B)\n",
    "\n",
    "                imgs_A.append(img_A)\n",
    "                imgs_B.append(img_B)\n",
    "            # normalizing the images \n",
    "            imgs_A = np.array(imgs_A)/127.5 - 1.\n",
    "            imgs_B = np.array(imgs_B)/127.5 - 1.\n",
    "\n",
    "            yield imgs_A, imgs_B\n",
    "def imread(path):\n",
    "        return scipy.misc.imread(path, mode='RGB').astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "        \"\"\"U-Net Generator\"\"\"\n",
    "\n",
    "        def conv2d(layer_input, filters, f_size=4, bn=True):\n",
    "            \"\"\"Layers used during downsampling\"\"\"\n",
    "            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            if bn:\n",
    "                d = BatchNormalization(momentum=0.8)(d)\n",
    "            return d\n",
    "\n",
    "        def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n",
    "            \"\"\"Layers used during upsampling\"\"\"\n",
    "            u = UpSampling2D(size=2)(layer_input)\n",
    "            u = Conv2D(filters, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)\n",
    "            if dropout_rate:\n",
    "                u = Dropout(dropout_rate)(u)\n",
    "            u = BatchNormalization(momentum=0.8)(u)\n",
    "            u = Concatenate()([u, skip_input]) #skip connection\n",
    "            return u\n",
    "\n",
    "        \n",
    "        d0 = Input(shape=img_shape)\n",
    "\n",
    "        # Downsampling\n",
    "        d1 = conv2d(d0, gf, bn=False)\n",
    "        d2 = conv2d(d1, gf*2)\n",
    "        d3 = conv2d(d2, gf*4)\n",
    "        d4 = conv2d(d3, gf*8)\n",
    "        d5 = conv2d(d4, gf*8)\n",
    "        d6 = conv2d(d5, gf*8)\n",
    "        d7 = conv2d(d6, gf*8)\n",
    "\n",
    "        # Upsampling\n",
    "        u1 = deconv2d(d7, d6, gf*8)\n",
    "        u2 = deconv2d(u1, d5, gf*8)\n",
    "        u3 = deconv2d(u2, d4, gf*8)\n",
    "        u4 = deconv2d(u3, d3, gf*4)\n",
    "        u5 = deconv2d(u4, d2, gf*2)\n",
    "        u6 = deconv2d(u5, d1, gf)\n",
    "\n",
    "        u7 = UpSampling2D(size=2)(u6)\n",
    "        output_img = Conv2D(channels, kernel_size=4, strides=1, padding='same', activation='tanh')(u7)\n",
    "\n",
    "        return Model(d0, output_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "        # a small function to make one layer of the discriminator\n",
    "        def d_layer(layer_input, filters, f_size=4, bn=True):\n",
    "            \"\"\"Discriminator layer\"\"\"\n",
    "            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            if bn:\n",
    "                d = BatchNormalization(momentum=0.8)(d)\n",
    "            return d\n",
    "\n",
    "        img_A = Input(shape=img_shape)\n",
    "        img_B = Input(shape=img_shape)\n",
    "\n",
    "        # Concatenate image and conditioning image by channels to produce input\n",
    "        combined_imgs = Concatenate(axis=-1)([img_A, img_B])\n",
    "\n",
    "        d1 = d_layer(combined_imgs, df, bn=False)\n",
    "        d2 = d_layer(d1, df*2)\n",
    "        d3 = d_layer(d2, df*4)\n",
    "        d4 = d_layer(d3, df*8)\n",
    "\n",
    "        validity = Conv2D(1, kernel_size=4, strides=1, padding='same')(d4)\n",
    "\n",
    "        return Model([img_A, img_B], validity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input shape\n",
    "img_rows = 128\n",
    "img_cols = 128\n",
    "channels = 3\n",
    "img_shape = (img_rows, img_cols, channels)\n",
    "\n",
    "\n",
    "# Calculate output shape of D (PatchGAN)\n",
    "patch = int(img_rows / 2**4)\n",
    "disc_patch = (patch, patch, 1)\n",
    "\n",
    "# Number of filters in the first layer of G and D\n",
    "gf = 64\n",
    "df = 64\n",
    "\n",
    "optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "# Build and compile the discriminator\n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(loss='mse',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "# Build the generator\n",
    "generator = build_generator()\n",
    "\n",
    "# Input images and their conditioning images\n",
    "img_A = Input(shape=img_shape)\n",
    "img_B = Input(shape=img_shape)\n",
    "\n",
    "# By conditioning on B generate a fake version of A\n",
    "fake_A = generator(img_B)\n",
    "\n",
    "# For the combined model we will only train the generator\n",
    "discriminator.trainable = False\n",
    "\n",
    "# Discriminators determines validity of translated images / condition pairs\n",
    "valid = discriminator([fake_A, img_B])\n",
    "\n",
    "combined = Model(inputs=[img_A, img_B], outputs=[valid, fake_A])\n",
    "combined.compile(loss=['mse', 'mae'],\n",
    "                              loss_weights=[1, 100],\n",
    "                              optimizer=optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images( dataset_name,epoch, batch_i):\n",
    "        \n",
    "        r, c = 3, 3\n",
    "\n",
    "        imgs_A, imgs_B = load_data(dataset_name,batch_size=3, is_val=True)\n",
    "        fake_A = generator.predict(imgs_B)\n",
    "\n",
    "        gen_imgs = np.concatenate([imgs_B, fake_A, imgs_A])\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        titles = ['Input', 'Output', 'Ground Truth']\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt])\n",
    "                axs[i, j].set_title(titles[i])\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train( dataset_name,epochs, batch_size=1, show_interval=10):\n",
    "\n",
    "        start_time = datetime.datetime.now()\n",
    "\n",
    "        # Adversarial loss ground truths\n",
    "        valid = np.ones((batch_size,) + disc_patch)\n",
    "        fake = np.zeros((batch_size,) + disc_patch)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for batch_i, (imgs_A, imgs_B) in enumerate(load_batch(dataset_name,batch_size)):\n",
    "\n",
    "                \n",
    "                #  Train Discriminator\n",
    "                \n",
    "\n",
    "                # Condition on B and generate a translated version\n",
    "                fake_A = generator.predict(imgs_B)\n",
    "\n",
    "                # Train the discriminators (original images = real / generated = Fake)\n",
    "                d_loss_real = discriminator.train_on_batch([imgs_A, imgs_B], valid)\n",
    "                d_loss_fake = discriminator.train_on_batch([fake_A, imgs_B], fake)\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "               \n",
    "                #  Train Generator\n",
    "                g_loss = combined.train_on_batch([imgs_A, imgs_B], [valid, imgs_A])\n",
    "\n",
    "                elapsed_time = datetime.datetime.now() - start_time\n",
    "                \n",
    "            # Plot the progress\n",
    "            if epoch%10==0:\n",
    "                  print (\"[Epoch %d/%d]  [D loss: %f, acc: %3d%%] [G loss: %f] time: %s\" % (epoch, epochs,\n",
    "                                                                        \n",
    "                                                                        d_loss[0], 100*d_loss[1],\n",
    "                                                                        g_loss[0],\n",
    "                                                                        elapsed_time))\n",
    "            # If at show interval => show generated image samples\n",
    "            if epoch % show_interval == 0:\n",
    "                    show_images(dataset_name,epoch, batch_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
