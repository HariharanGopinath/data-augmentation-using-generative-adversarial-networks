{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation Using Generative Adversarial Networks (GANs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook to perform data augmentation using Generative Adversarial Networks (GANs). This notebook must be run after running the notebook `prepare_dataset_and_create_project_structure.ipynb` that should be available in the same directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Ensure that the Cityscapes dataset is downloaded and placed in the directory named **dataset**. More information about the dataset can be found in the notebook `prepare_dataset_and_create_project_structure.ipynb` that should be available in the same directory. \n",
    "2. Run all the cells in the notebook `prepare_dataset_and_create_project_structure.ipynb` to understand the dataset, process the dataset, create the project structure required to ensure correct results from the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Check Python version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is crucial to ensure that the notebook runs on the correct version of Python to guarantee proper functionality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "assert (platform.python_version_tuple()[:2] >= ('3','7')), \"[ERROR] The notebooks are tested on Python 3.7 and higher. Please updated your Python to evaluate the code\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Check Notebook server has access to all required resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "dataset_folder = Path(\"dataset\")\n",
    "dataset_folder = Path.joinpath(Path.cwd(), dataset_folder)\n",
    "\n",
    "if not dataset_folder.exists():\n",
    "    raise FileNotFoundError(\"[ERROR] Add `{}` folder in the current directory (`{}`)\".format(dataset_folder.name, Path.cwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_preparation_notebook = Path(\"prepare_dataset_and_create_project_structure.ipynb\")\n",
    "dataset_preparation_notebook = Path.joinpath(Path.cwd(), dataset_preparation_notebook)\n",
    "\n",
    "if not dataset_preparation_notebook.exists():\n",
    "    raise FileNotFoundError(\"[ERROR] The notebook `{}` is unavailable in the current directory (`{}`). Please download and run the notebook `{}` before this notebook to ensure proper results.\".format(dataset_preparation_notebook.name, Path.cwd(), dataset_preparation_notebook.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = Path.joinpath(dataset_folder, \"test_dataset\")\n",
    "test_dataset_A = Path.joinpath(test_dataset, \"A\")\n",
    "test_dataset_B = Path.joinpath(test_dataset, \"B\")\n",
    "\n",
    "test_dataset_overall = [test_dataset, test_dataset_A, test_dataset_B]\n",
    "\n",
    "for dataset in test_dataset_overall:\n",
    "    if not dataset.exists():\n",
    "        raise FileNotFoundError(\"[ERROR] The folder `{}` is unavailable. Please run the notebook `prepare_dataset_and_create_project_structure.ipynb` available in the current directory (`{}`) before running this notebook.\".format(dataset.name, Path.cwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = Path.joinpath(dataset_folder, \"training_dataset\")\n",
    "training_dataset_A = Path.joinpath(training_dataset, \"A\")\n",
    "training_dataset_B = Path.joinpath(training_dataset, \"B\")\n",
    "\n",
    "training_dataset_overall = [training_dataset, training_dataset_A, training_dataset_B]\n",
    "\n",
    "for dataset in training_dataset_overall:\n",
    "    if not dataset.exists():\n",
    "        raise FileNotFoundError(\"[ERROR] The folder `{}` is unavailable. Please run the notebook `prepare_dataset_and_create_project_structure.ipynb` available in the current directory (`{}`) before running this notebook.\".format(dataset.name, Path.cwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validatation_dataset = Path.joinpath(dataset_folder, \"validatation_dataset\")\n",
    "validatation_dataset_A = Path.joinpath(validatation_dataset, \"A\")\n",
    "validatation_dataset_B = Path.joinpath(validatation_dataset, \"B\")\n",
    "\n",
    "validatation_dataset_overall = [validatation_dataset, validatation_dataset_A, validatation_dataset_B]\n",
    "\n",
    "for dataset in validatation_dataset_overall:\n",
    "    if not dataset.exists():\n",
    "        raise FileNotFoundError(\"[ERROR] The folder `{}` is unavailable. Please run the notebook `prepare_dataset_and_create_project_structure.ipynb` available in the current directory (`{}`) before running this notebook.\".format(dataset.name, Path.cwd()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the biggest bottlenecks in creating generalized deep learning models is a scarcity of high-quality data. The collection of high-quality data and its conversion is expensive. Most of the data collection methods are labor-intensive and error-prone, requiring considerable editing afterward to clean the data. Since large amounts of data are needed to achieve generalized deep learning models, standard data augmentation methods are routinely used to increase the dataset's generalizability. Data augmentation methods are also used when the datasets are imbalanced, improving the model's overall performance.\n",
    "\n",
    "Generative Adversarial Networks, popularly known as GANs, are a novel method for data augmentation. The generation of artificial training data can not only be instrumental in situations such as imbalanced data sets, but it can also be useful when the original dataset contains sensitive information. In such cases, it is then desirable to avoid using the original data as much as possible (For example, Medical data).\n",
    "\n",
    "This report proposes a GAN architecture based on a [paper](https://arxiv.org/abs/1611.07004) from UC Berkeley to perform data augmentation using the popular image-to-image translation method. Generative Adversarial Networks trained on these methods learn the mapping from an input image to an output image and learn a loss function to train this mapping. Therefore, this approach makes it possible to apply the same generic approach to problems that traditionally require very different loss formulations. In this particular report, we demonstrate that this approach can effectively synthesize photos from label maps. To evaluate the performance of the proposed GAN architecture, we utilize a standard dataset named Cityscapes. The Cityscapes Dataset focuses on semantic understanding of urban street scenes. The dataset contains 5000 images with detailed annotations and 20000 images with coarse annotations apart from the original images. Some sample images from the dataset are presented below:\n",
    "\n",
    "![Sample Image from Cityscape Dataset](https://www.cityscapes-dataset.com/wordpress/wp-content/uploads/2015/07/stuttgart02-2040x500.png)\n",
    "![Sample Image from Cityscape Dataset](https://www.cityscapes-dataset.com/wordpress/wp-content/uploads/2015/07/stuttgart00-2040x500.png)\n",
    "![Sample Image from Cityscape Dataset](https://www.cityscapes-dataset.com/wordpress/wp-content/uploads/2015/07/stuttgart04-2040x500.png)\n",
    "![Sample Image from Cityscape Dataset](https://www.cityscapes-dataset.com/wordpress/wp-content/uploads/2015/07/stuttgart01-2040x500.png)\n",
    "\n",
    "<center>Image Courtesy: Cityscapes Datatset (Link: https://www.cityscapes-dataset.com/)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Background Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the advancements in deep learning, the most striking successes have involved discriminative models, usually those that map a high-dimensional, rich sensory input to a class label. These striking successes have primarily been based on the backpropagation and dropout algorithms, using piecewise linear units. However, deep generative models have had less impact due to the challenges of approximating many probabilistic computations that occur due to the usage of piecewise linear units in the generative context. \n",
    "\n",
    "Generative Adversarial Networks, popularly known as GANs, is a machine learning framework class that sidesteps these difficulties by pitting the generative model against an adversary. In other words, a GAN is a machine learning framework where two neural networks compete against each other in a zero-sum game (i.e., one network's gain is the other network's loss). The two networks in a GAN can be considered as a generator and a discriminator. The generator learns to create images that look real, while the discriminator learns to tell real images apart from fakes. Competition in this game drives both networks to improve their models until the counterfeits are indistinguishable from the genuine images. An overview of the Generative Adversarial Network is represented below:\n",
    "\n",
    "![Overview of Generative Adversarial Network](https://developers.google.com/machine-learning/gan/images/gan_diagram.svg)\n",
    "\n",
    "<center>Image Courtesy: Google Developers (Link: https://developers.google.com/machine-learning/gan/gan_structure)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most significant characteristics of GANs is the lack of loss function. GANs learn the loss function to classify if the output image is real or fake while simultaneously training a generative model to minimize this loss. This property of GANs allows it to learn a loss that adapts to the data, making them a perfect solution to a multitude of tasks that traditionally would require very different kinds of loss functions such as image-to-image translation. The image-to-image translation is the task of translating one possible representation of a scene into another, given sufficient training data. Traditionally, each image-to-image translation task has been tackled with separate, special-purpose machinery, although the setting is always the same: predict pixels from pixels. The biggest problem with this approach is the need to formulate specialized loss functions that drive the neural network to do what we want â€“ e.g., output sharp, realistic images. Fortunately, the need for specialized loss functions is precisely eliminated with the use of GANs. \n",
    "\n",
    "In this report, we explore GANs in the conditional setting. Just as GANs learn a generative data model, conditional GANs (cGANs) learn a conditional generative model. In other words, just as GANs are generative models that learn a mapping from random noise vector $z$ to output image $y$, $G:\\,z\\,\\rightarrow\\,y$, the conditional GANs (cGANs) learn a mapping from observed image $x$ and random noise vector $z$, to output image $y$,$G:\\,\\{x,\\, z\\}\\,\\rightarrow\\,y$. This ability to map from an observed image to an output image makes cGANs fitting for image-to-image translation tasks, where we condition on an input image and generate a corresponding output image. \n",
    "\n",
    "Finally, to understand the objective function of the conditional GANs (i.e., cGANS), let us first examine the objective function of a traditional GAN. The equation below refers to the objective function of a traditional GAN.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{L}_{GAN}(G,D)\\,=\\,&\\mathbb{E}_{y}[logD(y)]\\,+\\, \\\\\n",
    "&\\mathbb{E}_{x\\,,\\,z}[log(1\\,-\\,D(G(x\\,,\\,z)))]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "It can be observed from the Equation above that Generator $G$ tries to minimize the objective function against an adversarial $D$ that tries to maximize it. Therefore, the optimal value of G can be represented as the equation below.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "G^{*}\\,=\\,arg\\,min_{G}\\,max_{D}\\,\\mathbb{L}_{GAN}(G\\,,\\,D)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Based on the Equation above, the objective function of a conditional GAN can be represented as Equation below.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{L}_{cGAN}(G,D)\\,=\\,&\\mathbb{E}_{x\\,,\\,y}[logD(x\\,,\\,y)]\\,+\\, \\\\\n",
    "&\\mathbb{E}_{x\\,,\\,z}[log(1\\,-\\,D(x\\,,\\,G(x\\,,\\,z)))]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Similar to the equation above, Generator $G$ tries to minimize the objective function against an adversarial $D$ that tries to maximize it. By training both $G$ and $D$ on the objective function $\\mathbb{L}_{cGAN}(G\\,,\\,D)$ simultaneously, the optimal values for both the parameters can be calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Proposed Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import numpy as np \n",
    "import scipy as sp\n",
    "from imageio import imread\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_randomized_data(sample_size, dataset_type=\"training\"):\n",
    "    \"\"\"\n",
    "    Function to draw randomized data samples from dataset. This function would be required while displaying training output.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sample_size  : Number of data samples to be loaded (type=int)\n",
    "    dataset_type : Type of dataset to draw randomized data samples from (type=str) \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    random_ground_truth_data  : Randomized \"Ground Truth Data\" drawn from the dataset \"dataset_type\" (type=list)\n",
    "    random_input_data         : Randomized \"Input Data\" drawn from the dataset \"dataset_type\" (type=list)\n",
    "    \"\"\"\n",
    "    \n",
    "    if(dataset_type == \"training\"):\n",
    "        ground_truth_dataset_expr = str(training_dataset_A) + \"\\\\**\\\\*.jpg\"\n",
    "        input_dataset_expr = str(training_dataset_B) + \"\\\\**\\\\*.jpg\"\n",
    "    else:\n",
    "        ground_truth_dataset_expr = str(validatation_dataset_A) + \"\\\\**\\\\*.jpg\"\n",
    "        input_dataset_expr = str(validatation_dataset_B) + \"\\\\**\\\\*.jpg\"\n",
    "        \n",
    "    ground_truth_dataset_paths = glob.glob(ground_truth_dataset_expr, recursive=True)\n",
    "    ground_truth_dataset_paths = sorted(ground_truth_dataset_paths)\n",
    "   \n",
    "    input_dataset_paths = glob.glob(input_dataset_expr, recursive=True)\n",
    "    input_dataset_paths = sorted(input_dataset_paths)\n",
    "    \n",
    "    assert (len(ground_truth_dataset_paths) == len(input_dataset_paths)), (\"[ERROR] Datasets Modified After Preprocessing! Please reload the dataset from the beginning!\")\n",
    "        \n",
    "    random_data_samples = np.random.choice(range(len(ground_truth_dataset_paths)), sample_size) # Pick any random data from the dataset\n",
    "   \n",
    "    random_ground_truth_data = []\n",
    "    random_input_data = []\n",
    "    \n",
    "    for random_sample in random_data_samples:\n",
    "        \n",
    "        ground_truth_data_path = ground_truth_dataset_paths[random_sample]\n",
    "        input_data_path = input_dataset_paths[random_sample]\n",
    "        \n",
    "        ground_truth_data = imread(ground_truth_data_path)\n",
    "        input_data = imread(input_data_path)\n",
    "        \n",
    "        if((dataset_type != \"training\") and (np.random.random() > 0.5)):\n",
    "            ground_truth_data = np.fliplr(ground_truth_data)\n",
    "            input_data = np.fliplr(input_data)\n",
    "        \n",
    "        random_ground_truth_data.append(ground_truth_data)\n",
    "        random_input_data.append(input_data)\n",
    "        \n",
    "    # random_ground_truth_data = np.array(random_ground_truth_data)/127.5 - 1\n",
    "    # random_input_data = np.array(random_input_data)/127.5 - 1\n",
    "        \n",
    "    return random_ground_truth_data, random_input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_batches(batch_size, dataset_type=\"training\"):\n",
    "    \"\"\"\n",
    "    Function to load batches. This function would be required while training the neural network.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    batch_size   : Size of the batches to be loaded (type=int)\n",
    "    dataset_type : Type of dataset to load batches from (type=str) \n",
    "    \n",
    "    Yeilds (Returns a Generator and Not an Iterator [IMPORTANT!])\n",
    "    ------\n",
    "    ground_truth_data_batch  : Batch of \"Ground Truth Data\" loaded from the dataset \"dataset_type\" (type=list)\n",
    "    input_data_batch         : Batch of \"Input Data\" loaded from the dataset \"dataset_type\" (type=list)\n",
    "    \"\"\"\n",
    "    \n",
    "    if(dataset_type == \"training\"):\n",
    "        ground_truth_dataset_expr = str(training_dataset_A) + \"\\\\**\\\\*.jpg\"\n",
    "        input_dataset_expr = str(training_dataset_B) + \"\\\\**\\\\*.jpg\"\n",
    "    else:\n",
    "        ground_truth_dataset_expr = str(validatation_dataset_A) + \"\\\\**\\\\*.jpg\"\n",
    "        input_dataset_expr = str(validatation_dataset_B) + \"\\\\**\\\\*.jpg\"\n",
    "        \n",
    "    ground_truth_dataset_paths = glob.glob(ground_truth_dataset_expr, recursive=True)\n",
    "    ground_truth_dataset_paths = sorted(ground_truth_dataset_paths)\n",
    "   \n",
    "    input_dataset_paths = glob.glob(input_dataset_expr, recursive=True)\n",
    "    input_dataset_paths = sorted(input_dataset_paths)\n",
    "    \n",
    "    assert (len(ground_truth_dataset_paths) == len(input_dataset_paths)), (\"[ERROR] Datasets Modified After Preprocessing! Please reload the dataset from the beginning!\")\n",
    "    \n",
    "    print(len(ground_truth_dataset_paths))\n",
    "    print(int(len(ground_truth_dataset_paths)/batch_size))\n",
    "    \n",
    "    for batch_iterator in range(int(len(ground_truth_dataset_paths)/batch_size) - 1):\n",
    "\n",
    "        ground_truth_dataset_paths_batch = ground_truth_dataset_paths[(batch_iterator)*batch_size:(batch_iterator+1)*batch_size]\n",
    "        input_dataset_paths_batch = input_dataset_paths[(batch_iterator)*batch_size:(batch_iterator+1)*batch_size]\n",
    "\n",
    "        ground_truth_data_batch = []\n",
    "        input_data_batch = []\n",
    "\n",
    "        for (ground_truth_data_path_iterator, input_data_path_iterator) in zip(ground_truth_dataset_paths_batch, input_dataset_paths_batch):\n",
    "\n",
    "            ground_truth_data = imread(ground_truth_data_path_iterator)\n",
    "            input_data = imread(input_data_path_iterator)\n",
    "           \n",
    "            if((dataset_type != \"training\") and (np.random.random() > 0.5)):\n",
    "                ground_truth_data = np.fliplr(ground_truth_data)\n",
    "                input_data = np.fliplr(input_data)\n",
    "        \n",
    "            ground_truth_data_batch.append(ground_truth_data)\n",
    "            input_data_batch.append(input_data)\n",
    "            \n",
    "        # random_ground_truth_data = np.array(random_ground_truth_data)/127.5 - 1\n",
    "        # random_input_data = np.array(random_input_data)/127.5 - 1\n",
    "        \n",
    "        yield ground_truth_data_batch, input_data_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define cGAN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "    \"\"\"\n",
    "    Closure Function to Build A Generator for cGAN.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    None\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Model   : Generator Model for cGAN (type=tf.keras.models.Model)\n",
    "    \"\"\"\n",
    "    def conv2d(input_layer, filters, filter_shape=4, batch_normalization=True):\n",
    "        \"\"\"\n",
    "        Nested Function to create a 2D-Convolution Layer\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        input_layer         : Input Layer to the 2D-Convolution Layer\n",
    "        filters             : Number of filters in the convolution\n",
    "        filter_shape        : Size of the 2D convolution window\n",
    "        batch_normalization : Flag to set Batch Normalization\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        d    : 2D-Convolution Layer\n",
    "        \"\"\"\n",
    "        d = Conv2D(filters, kernel_size=filter_shape, strides=2, padding='same')(input_layer)\n",
    "        d = LeakyReLU(alpha=0.2)(d)\n",
    "        if(batch_normalization == True):\n",
    "            d = BatchNormalization(momentum=0.8)(d)\n",
    "            \n",
    "        return d\n",
    "    \n",
    "    def deconv2d(input_layer, skip_input, filters, filter_shape=4, dropout_rate=0):\n",
    "        \"\"\"\n",
    "        Nested Function to create a 2D-Deconvolution Layer\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        input_layer   : Input Layer to the 2D-Convolution Layer\n",
    "        skip_input    : Input Layer to be Skipped (i.e., Inputs of this layer are concatenated)\n",
    "        filters       : Number of filters in the convolution\n",
    "        filter_shape  : Size of the 2D convolution window\n",
    "        dropout_rate  : Dropout Rate\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        u    : 2D-Deconvolution Layer\n",
    "        \"\"\"\n",
    "        u = UpSampling2D(size=2)(input_layer)\n",
    "        u = Conv2D(filters, kernel_size=filter_shape, strides=1, padding='same', activation='relu')(u)\n",
    "        if dropout_rate:\n",
    "            u = Dropout(dropout_rate)(u)\n",
    "        u = BatchNormalization(momentum=0.8)(u)\n",
    "        u = Concatenate()([u, skip_input]) #skip connection\n",
    "        \n",
    "        return u\n",
    "    \n",
    "    \n",
    "    d0 = Input(shape=image_shape)\n",
    "\n",
    "    # Downsampling\n",
    "    d1 = conv2d(d0, gf, batch_normalization=False)\n",
    "    d2 = conv2d(d1, gf*2)\n",
    "    d3 = conv2d(d2, gf*4)\n",
    "    d4 = conv2d(d3, gf*8)\n",
    "    d5 = conv2d(d4, gf*8)\n",
    "    d6 = conv2d(d5, gf*8)\n",
    "    d7 = conv2d(d6, gf*8)\n",
    "\n",
    "    # Upsampling\n",
    "    u1 = deconv2d(d7, d6, gf*8)\n",
    "    u2 = deconv2d(u1, d5, gf*8)\n",
    "    u3 = deconv2d(u2, d4, gf*8)\n",
    "    u4 = deconv2d(u3, d3, gf*4)\n",
    "    u5 = deconv2d(u4, d2, gf*2)\n",
    "    u6 = deconv2d(u5, d1, gf)\n",
    "\n",
    "    u7 = UpSampling2D(size=2)(u6)\n",
    "    generated_image = Conv2D(channels, kernel_size=4, strides=1, padding='same', activation='tanh')(u7)\n",
    "\n",
    "    return Model(d0, generated_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "    \"\"\"\n",
    "    Closure Function to Build A Discriminator for cGAN.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    None\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Model   : Discriminator Model for cGAN (type=tf.keras.models.Model)\n",
    "    \"\"\"\n",
    "    def d_layer(input_layer, filters, filter_shape=4, batch_normalization=True):\n",
    "        \"\"\"\n",
    "        Nested Function to create a single layer discriminator\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        input_layer         : Input Layer to the 2D-Convolution Layer\n",
    "        filters             : Number of filters in the convolution\n",
    "        filter_shape        : Size of the 2D convolution window\n",
    "        batch_normalization : Flag to set Batch Normalization\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        d    : Discriminator layer\n",
    "        \"\"\"\n",
    "        d = Conv2D(filters, kernel_size=filter_shape, strides=2, padding='same')(input_layer)\n",
    "        d = LeakyReLU(alpha=0.2)(d)\n",
    "        if batch_normalization:\n",
    "            d = BatchNormalization(momentum=0.8)(d)\n",
    "            \n",
    "        return d\n",
    "    \n",
    "    ground_truth_image = Input(shape=image_shape)\n",
    "    input_image = Input(shape=image_shape)\n",
    "\n",
    "    # Concatenate image and conditioning image by channels to produce input\n",
    "    combined_images = Concatenate(axis=-1)([ground_truth_image, input_image])\n",
    "\n",
    "    d1 = d_layer(combined_images, df, batch_normalization=False)\n",
    "    d2 = d_layer(d1, df*2)\n",
    "    d3 = d_layer(d2, df*4)\n",
    "    d4 = d_layer(d3, df*8)\n",
    "\n",
    "    validity = Conv2D(1, kernel_size=4, strides=1, padding='same')(d4)\n",
    "\n",
    "    return Model([ground_truth_image, input_image], validity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input shape\n",
    "image_rows = 128\n",
    "image_columns = 128\n",
    "channels = 3\n",
    "image_shape = (image_rows, image_columns, channels)\n",
    "\n",
    "# Calculate output shape of D (PatchGAN)\n",
    "patch = int(image_rows / 2**4)\n",
    "discriminator_patch = (patch, patch, 1)\n",
    "\n",
    "# Number of filters in the first layer of G and D\n",
    "gf = 64\n",
    "df = 64\n",
    "\n",
    "optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "# Build and compile the discriminator\n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Build the generator\n",
    "generator = build_generator()\n",
    "\n",
    "# Input images and their conditioning images\n",
    "ground_truth_image = Input(shape=image_shape)\n",
    "input_image = Input(shape=image_shape)\n",
    "\n",
    "# By conditioning on B generate a fake version of A\n",
    "generated_ground_truth_image = generator(input_image)\n",
    "\n",
    "# For the combined model we will only train the generator\n",
    "discriminator.trainable = False\n",
    "\n",
    "# Discriminators determines validity of translated images / condition pairs\n",
    "valid = discriminator([generated_ground_truth_image, input_image])\n",
    "\n",
    "gan = Model(inputs=[ground_truth_image, input_image], outputs=[valid, generated_ground_truth_image])\n",
    "gan.compile(loss=['mse', 'mae'], loss_weights=[1, 100], optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display output\n",
    "\n",
    "def display_output(output_data_type, number_of_outputs=3):\n",
    "    \n",
    "    real_ground_truth_images, input_images = draw_randomized_data(number_of_outputs, dataset_type=output_data_type)\n",
    "    \n",
    "    # print(real_ground_truth_images)\n",
    "    print(type(real_ground_truth_images))\n",
    "    print(len(real_ground_truth_images))\n",
    "    print(real_ground_truth_images[0].shape)\n",
    "    \n",
    "    # print(input_images)\n",
    "    print(type(input_images))\n",
    "    print(len(input_images))\n",
    "    print(input_images[0].shape)\n",
    "    \n",
    "    generated_ground_truth_images = generator.predict(input_images)\n",
    "    \n",
    "    print(generated_ground_truth_images)\n",
    "    print(type(generated_ground_truth_images))\n",
    "    print(generated_ground_truth_images.shape)\n",
    "    \n",
    "    overall_generated_images = np.concatenate(input_images, generated_ground_truth_images, real_ground_truth_images)\n",
    "    \n",
    "    print(overall_generated_images)\n",
    "    print(type(overall_generated_images))\n",
    "    print(overall_generated_images.shape)\n",
    "    \n",
    "    # Rescale Image\n",
    "    overall_generated_images = 0.5 * overall_generated_images + 0.5\n",
    "    \n",
    "    titles = [\"Input\", \"Output\", \"Ground Truth\"]\n",
    "    \n",
    "    f, axarr = plt.subplots(3, number_of_outputs, figsize=(20,30))\n",
    "    \n",
    "    for row_iterator in range(3):\n",
    "        for column_iterator in range(number_of_outputs):\n",
    "            axarr[row_iterator, column_iterator].imshow(overall_generated_images[row_iterator + column_iterator])\n",
    "            axarr[row_iterator, column_iterator].set_title(titles[row_iterator])\n",
    "            axarr[row_iterator, column_iterator].axis(\"off\")\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform training\n",
    "def train_neural_network(epochs, batch_size, display_interval=10):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    print(\"cGAN Training started at {}\".format(time.asctime(time.localtime(start_time))))\n",
    "    \n",
    "    # Calculate Batch Shape and Target Values\n",
    "    batch_shape = (batch_size,) + discriminator_patch    # Joining Tuples to create a batch shape of dimensions: (batch, height, width, channels)\n",
    "    target_real_images = np.ones(batch_shape)\n",
    "    target_fake_images = np.zeros(batch_shape)\n",
    "    \n",
    "    # Initialize Generaor and Discriminator Losses\n",
    "    generator_loss_overall = []\n",
    "    discriminator_loss_overall = []\n",
    "    \n",
    "    for current_epoch in range(epochs):\n",
    "        \n",
    "        for batch_iterator, (ground_truth_images, input_images) in enumerate(load_batches(batch_size)):\n",
    "        \n",
    "            # Generate Fake Ground Truth Images\n",
    "            fake_ground_truth_images = generator.predict(input_images)\n",
    "            \n",
    "            # Train Discriminator\n",
    "            discriminator_loss_real_ground_truth = discriminator.train_on_batch([ground_truth_images, input_images], target_real_images)\n",
    "            \n",
    "            print(discriminator_loss_real_ground_truth)\n",
    "            print(type(discriminator_loss_real_ground_truth))\n",
    "            print(discriminator_loss_real_ground_truth.shape)\n",
    "            \n",
    "            \n",
    "            discriminator_loss_fake_ground_truth = discriminator.train_on_batch([ground_truth_images, input_images], target_fake_images)\n",
    "            \n",
    "            print(discriminator_loss_fake_ground_truth)\n",
    "            print(type(discriminator_loss_fake_ground_truth))\n",
    "            print(discriminator_loss_fake_ground_truth.shape)\n",
    "            \n",
    "            discriminator_loss_current = 0.5*np.add(discriminator_loss_real_ground_truth, discriminator_loss_fake_ground_truth)\n",
    "            \n",
    "            print(discriminator_loss_current)\n",
    "            print(type(discriminator_loss_current))\n",
    "            print(discriminator_loss_current.shape)\n",
    "            \n",
    "            # Train Generator\n",
    "            generator_loss_current = gan.train_on_batch([ground_truth_images, input_images], [target_fake_images, ground_truth_images])\n",
    "            \n",
    "            print(generator_loss_current)\n",
    "            print(type(generator_loss_current))\n",
    "            print(generator_loss_current.shape)\n",
    "        \n",
    "            # Calculate Elapsed Time\n",
    "            current_time = time.time()    \n",
    "            elapsed_time = current_time - start_time  \n",
    "            \n",
    "        # Append Losses\n",
    "        generator_loss_overall.append(generator_loss_current)\n",
    "        discriminator_loss_overall.append(discriminator_loss_current)\n",
    "        \n",
    "        # Display Results\n",
    "        print(f\"Epoch: {current_epoch + 1}/{epochs}, Generator Loss: {generator_loss_current:.2f}, Discriminator Loss: {discriminator_loss_current:.2f}, Elapsed Time: {elapsed_time:.2f}\")\n",
    "        \n",
    "        # Display Images\n",
    "        if(current_epoch % display_interval == 0):\n",
    "            display_output(output_data_type=\"training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
